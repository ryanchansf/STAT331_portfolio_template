---
title: "The Power of Efficiency"
format: html
editor: visual
---

As we’ve said in the class efficiency is a pivotal component of statistical computing (and data science). In this essay, give an explanation of what that term “efficiency” means in relation to statistical computing and describe some places where you encountered efficiency and understood its importance. Your essay should address the following questions:

-   What is the definition of “efficiency”?

-   What does efficiency look like in statistical computing / data science?

-   What does efficiency allow you to do?

-   Why is efficiency important?

-   Where did you encounter efficiency, and what were some [“a-ha” moments](https://www.merriam-webster.com/dictionary/aha%20moment) you had about efficiency? (For the latter, tie each a-ha moment to an artifact in the portfolio.)

Efficiency is the ability to process and interpret data with optimal computational resources and time while enforcing accuracy. In this class, we've seen efficiency in statistical computing through streamlined code, which can come in the form of functions, limiting redundant function calls, and writing programs that allow for reproducibility. Having these standards allows us to process data faster and reduce unnecessary computation, which puts deriving statistical insights at the forefront of our programs. Efficiency is especially important when working with large data sets that require various tasks such as data cleaning and other operations. For instance, efficiency became prevalent in Lab 4 Question 6, where we had to integrate both data manipulation with an efficient pipeline to plot findings in one dplyr pipeline. In this scenario, rather than doing the plotting and data cleaning separately by assigning the resulting data frame to a variable, a single pipeline was used to increase efficiency. I encountered another "a-ha" moment in Lab 5 when I did the opposite to increase efficiency. Rather than executing a single pipeline to find the suspect, I achieved efficiency by actually saving the work in one pipeline to a variable in order to avoid filtering by a hardcoded ID in multiple steps. This taught me that separating data transformations into separate chunks can actually increase efficiency rather than reduce it while also providing clarity and standards in code. Lastly, an "a-ha" moment for me was realizing when and why to use map() instead of across() for applying functions to multiple columns. While across() is good for vectorized operations, I discovered in Lab 8 Question 4 that map(), specifically map_int(), provides a more direct use of efficiency for applying functions column-by-column. This streamlined operation helped me understand the benefits and use-cases of functional programming, ensuring better computational efficiency.
